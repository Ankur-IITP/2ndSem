# -*- coding: utf-8 -*-
"""EngHindi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EMVE2jNxuGwNskCM3FxyCxDSPKYooHsS
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, Dot, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# Load Dataset
dataset_path = "/content/eng to hin.txt"  #
data = pd.read_csv(dataset_path, delimiter='\t', names=['english', 'hindi'])

# Preprocess Data
data['hindi'] = data['hindi'].apply(lambda x: f"<start> {x.strip()} <end>")
train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)

def tokenize_and_pad(data, num_words=10000, max_len=None):
    tokenizer = Tokenizer(num_words=num_words, filters='')
    tokenizer.fit_on_texts(data)
    sequences = tokenizer.texts_to_sequences(data)
    if max_len is None:
        max_len = max(len(seq) for seq in sequences)
    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')
    return tokenizer, padded_sequences, max_len

eng_tokenizer, eng_train, eng_max_len = tokenize_and_pad(train_data['english'])
hindi_tokenizer, hindi_train, hindi_max_len = tokenize_and_pad(train_data['hindi'])

eng_vocab_size = len(eng_tokenizer.word_index) + 1
hindi_vocab_size = len(hindi_tokenizer.word_index) + 1

# Split Decoder Input and Target
decoder_input_data = hindi_train[:, :-1]
decoder_target_data = hindi_train[:, 1:]

# Model Definition
# Encoder
encoder_inputs = Input(shape=(eng_max_len,))
enc_emb = Embedding(eng_vocab_size, 256, mask_zero=True)(encoder_inputs)
encoder_lstm = LSTM(256, return_sequences=True, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)

# Decoder
decoder_inputs = Input(shape=(hindi_max_len - 1,))
dec_emb = Embedding(hindi_vocab_size, 256, mask_zero=True)(decoder_inputs)
decoder_lstm = LSTM(256, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])

# Attention Mechanism
attention_scores = Dot(axes=[2, 2])([decoder_outputs, encoder_outputs])  # Alignment scores
attention_weights = Activation('softmax')(attention_scores)  # Attention weights
context_vector = Dot(axes=[2, 1])([attention_weights, encoder_outputs])  # Context vector
concat_attention = Concatenate(axis=-1)([context_vector, decoder_outputs])

# Output Layer
outputs = Dense(hindi_vocab_size, activation='softmax')(concat_attention)

# Model Compilation
model = Model([encoder_inputs, decoder_inputs], outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
model.summary()

# Train the Model
model.fit(
    [eng_train, decoder_input_data],
    decoder_target_data,
    batch_size=64,
    epochs=10,
    validation_split=0.1
)

# Define Encoder Model for Inference
encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])

# Define Decoder Model for Inference
decoder_state_input_h = Input(shape=(256,))
decoder_state_input_c = Input(shape=(256,))
decoder_hidden_state_input = Input(shape=(eng_max_len, 256))

# Embedding layer for decoder
dec_emb2 = Embedding(hindi_vocab_size, 256, mask_zero=True)(decoder_inputs)

# LSTM layer for decoder
decoder_outputs2, state_h2, state_c2 = decoder_lstm(
    dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c]
)

# Attention mechanism for inference
attention_scores2 = Dot(axes=[2, 2])([decoder_outputs2, decoder_hidden_state_input])
attention_weights2 = Activation('softmax')(attention_scores2)
context_vector2 = Dot(axes=[2, 1])([attention_weights2, decoder_hidden_state_input])
concat_attention2 = Concatenate(axis=-1)([context_vector2, decoder_outputs2])

# Output layer for the decoder
decoder_outputs_final = Dense(hindi_vocab_size, activation='softmax')(concat_attention2)

# Define the decoder model
decoder_model = Model(
    [decoder_inputs, decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],
    [decoder_outputs_final, state_h2, state_c2]
)

# Translate Function for Inference
def translate_sentence(input_sentence):
    input_seq = pad_sequences(
        eng_tokenizer.texts_to_sequences([input_sentence]),
        maxlen=eng_max_len,
        padding='post'
    )
    encoder_output, state_h, state_c = encoder_model.predict(input_seq)

    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = hindi_tokenizer.word_index['<start>']

    decoded_sentence = ''
    stop_condition = False
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict(
            [target_seq, encoder_output, state_h, state_c]
        )

        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_word = hindi_tokenizer.index_word.get(sampled_token_index, '')
        if sampled_word == '<end>' or len(decoded_sentence.split()) > 50:
            stop_condition = True
        else:
            decoded_sentence += ' ' + sampled_word

        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index
        state_h, state_c = h, c
    return decoded_sentence

# Example Translation Test
for i in range(5):
    eng_sentence = val_data.iloc[i]['english']
    print(f"Input: {eng_sentence}")
    print(f"Output: {translate_sentence(eng_sentence)}")

custom_sentence = "Bank"  # Replace with any English sentence
print(f"Input (English): {custom_sentence}")
translated_sentence = translate_sentence(custom_sentence)
print(f"Output (Hindi): {translated_sentence}")