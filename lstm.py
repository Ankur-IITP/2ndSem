# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h6ARRaGzMB_Vh8Oj5zEmfl-Cp0Avq6l-
"""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense


### Load Data
# Load any English text data
with open('https://raw.githubusercontent.com/Ankur-IITP/2ndSem/refs/heads/main/data.txt', 'r') as f:
 text = f.read()


### Preprocessing


# Create vocabulary
vocab = set(text)
vocab_size = len(vocab)


# Create character-to-index mapping
char_to_idx = {char: idx for idx, char in enumerate(vocab)}


# Convert text to indices
indices = [char_to_idx[char] for char in text]


### Data Preparation


# Sequence length
seq_len = 100


# Create training sequences
sequences = []
next_chars = []
for i in range(len(indices) - seq_len):
    sequences.append(indices[i:i + seq_len])
    next_chars.append(indices[i + seq_len])


# Convert sequences and next characters to numpy arrays
sequences = np.array(sequences)
next_chars = np.array(next_chars)


# One-hot encoding for sequences
sequences_one_hot = np.zeros((sequences.shape[0], seq_len, vocab_size))
for i, seq in enumerate(sequences):
    sequences_one_hot[i, np.arange(seq_len), seq] = 1


### Model Definition


# Define LSTM model
model = Sequential()
model.add(LSTM(64, input_shape=(seq_len, vocab_size)))
model.add(Dense(vocab_size, activation='softmax'))
#model.add(Masking(mask_value=0, input_shape=(None, vocab_size)))


# Compile model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])


### Training


# Train model
model.fit(sequences_one_hot, next_chars, epochs=100, batch_size=128)


### Prediction


# Define prediction function
def predict_next_char(model, input_seq):
    input_seq_padded = np.pad(input_seq, (0, seq_len - len(input_seq)), mode='constant')
    input_seq_one_hot = np.zeros((1, seq_len, vocab_size))
    input_seq_one_hot[0, np.arange(seq_len), input_seq_padded] = 1
    probs = model.predict(input_seq_one_hot)
    return np.argmax(probs)


# Example usage
input_seq = [char_to_idx['h'], char_to_idx['e'], char_to_idx['l'], char_to_idx['l'], char_to_idx['o']]
next_char_idx = predict_next_char(model, input_seq)
next_char = [char for char, idx in char_to_idx.items() if idx == next_char_idx][0]
print(f"Next character: {next_char}")


### Generation


# Define text generation function
# Create inverse dictionary
idx_to_char = {idx: char for char, idx in char_to_idx.items()}

def generate_text(model, start_seq, max_len):
    generated_text = ''.join([idx_to_char[idx] for idx in start_seq])
    for _ in range(max_len):
        next_char_idx = predict_next_char(model, start_seq)
        generated_text += idx_to_char[next_char_idx]
        start_seq = start_seq[1:] + [next_char_idx]
    return generated_text


# Example usage
#start_seq = [char_to_idx['h'], char_to_idx['e'], char_to_idx['l'], char_to_idx['l'], char_to_idx['o']]
start_seq = [char_to_idx['h'], char_to_idx['e'], char_to_idx['l']]
generated_text = generate_text(model, start_seq, 4)
print(f"Generated text: {generated_text}")

start_seq = [char_to_idx['h'], char_to_idx['e'], char_to_idx['l'],char_to_idx['l'],char_to_idx['o']]
generated_text = generate_text(model, start_seq, 4)
print(f"Generated text: {generated_text}")